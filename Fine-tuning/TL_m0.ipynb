{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TL-m0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8W1SOj5kYrF"
      },
      "source": [
        "## Fine-tuning\n",
        "- This notebook contains code for the training of target task regressor without any pre-trained or fine-tuned weights, i.e., no transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3eWNliYk0c2"
      },
      "source": [
        "#### Install RDKit on Google colaboratory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxcQYryPkWUn"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import requests\n",
        "import subprocess\n",
        "import shutil\n",
        "from logging import getLogger, StreamHandler, INFO\n",
        "\n",
        "\n",
        "logger = getLogger(__name__)\n",
        "logger.addHandler(StreamHandler())\n",
        "logger.setLevel(INFO)\n",
        "\n",
        "\n",
        "def install(\n",
        "        chunk_size=4096,\n",
        "        file_name=\"Miniconda3-latest-Linux-x86_64.sh\",\n",
        "        url_base=\"https://repo.continuum.io/miniconda/\",\n",
        "        conda_path=os.path.expanduser(os.path.join(\"~\", \"miniconda\")),\n",
        "        rdkit_version=None,\n",
        "        add_python_path=True,\n",
        "        force=False):\n",
        "    \"\"\"install rdkit from miniconda\n",
        "    ```\n",
        "    import rdkit_installer\n",
        "    rdkit_installer.install()\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    python_path = os.path.join(\n",
        "        conda_path,\n",
        "        \"lib\",\n",
        "        \"python{0}.{1}\".format(*sys.version_info),\n",
        "        \"site-packages\",\n",
        "    )\n",
        "\n",
        "    if add_python_path and python_path not in sys.path:\n",
        "        logger.info(\"add {} to PYTHONPATH\".format(python_path))\n",
        "        sys.path.append(python_path)\n",
        "\n",
        "    if os.path.isdir(os.path.join(python_path, \"rdkit\")):\n",
        "        logger.info(\"rdkit is already installed\")\n",
        "        if not force:\n",
        "            return\n",
        "\n",
        "        logger.info(\"force re-install\")\n",
        "\n",
        "    url = url_base + file_name\n",
        "    python_version = \"{0}.{1}.{2}\".format(*sys.version_info)\n",
        "\n",
        "    logger.info(\"python version: {}\".format(python_version))\n",
        "\n",
        "    if os.path.isdir(conda_path):\n",
        "        logger.warning(\"remove current miniconda\")\n",
        "        shutil.rmtree(conda_path)\n",
        "    elif os.path.isfile(conda_path):\n",
        "        logger.warning(\"remove {}\".format(conda_path))\n",
        "        os.remove(conda_path)\n",
        "\n",
        "    logger.info('fetching installer from {}'.format(url))\n",
        "    res = requests.get(url, stream=True)\n",
        "    res.raise_for_status()\n",
        "    with open(file_name, 'wb') as f:\n",
        "        for chunk in res.iter_content(chunk_size):\n",
        "            f.write(chunk)\n",
        "    logger.info('done')\n",
        "\n",
        "    logger.info('installing miniconda to {}'.format(conda_path))\n",
        "    subprocess.check_call([\"bash\", file_name, \"-b\", \"-p\", conda_path])\n",
        "    logger.info('done')\n",
        "\n",
        "    logger.info(\"installing rdkit\")\n",
        "    subprocess.check_call([\n",
        "        os.path.join(conda_path, \"bin\", \"conda\"),\n",
        "        \"install\",\n",
        "        \"--yes\",\n",
        "        \"-c\", \"rdkit\",\n",
        "        \"python=={}\".format(python_version),\n",
        "        \"rdkit\" if rdkit_version is None else \"rdkit=={}\".format(rdkit_version)])\n",
        "    logger.info(\"done\")\n",
        "\n",
        "    import rdkit\n",
        "    logger.info(\"rdkit-{} installation finished!\".format(rdkit.__version__))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    install()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiA0VfMkk3Rj"
      },
      "source": [
        "Import the important libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRjp0Uryk5_L"
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "from rdkit import RDLogger \n",
        "RDLogger.DisableLog('rdApp.*') # switch off RDKit warning messages\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from fastai import *\n",
        "from fastai.text import *\n",
        "from fastai.vision import *\n",
        "\n",
        "import numpy as np\n",
        "import threading\n",
        "import random\n",
        "from sklearn.utils import shuffle "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47_kCIVnlCLz"
      },
      "source": [
        "Set the seed value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kd3TcE-5k85a"
      },
      "source": [
        "def random_seed(seed_value, use_cuda):\n",
        "    np.random.seed(seed_value) # cpu vars\n",
        "    torch.manual_seed(seed_value) # cpu  vars\n",
        "    random.seed(seed_value) # Python\n",
        "    if use_cuda: \n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value) # gpu vars\n",
        "        torch.backends.cudnn.deterministic = True  #needed\n",
        "        torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4_geG-0lFNq"
      },
      "source": [
        "# Data\n",
        "Mount Google Drive to Google Colab to access the google drive files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36xSIODDXPXc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSOQJTqylKFC"
      },
      "source": [
        "# Create a path to save the results\n",
        "\n",
        "data_path = Path('/content/gdrive/My Drive/results')\n",
        "name = 'regressor'\n",
        "path = data_path/name\n",
        "path.mkdir(exist_ok=True, parents=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89OgA-FvlPp6"
      },
      "source": [
        "data = pd.read_csv('/content/gdrive/My Drive/data/target-reactions.csv')\n",
        "print('Dataset:', ee_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6lknqM4lU2S"
      },
      "source": [
        "## Training the target task regressor from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0IuwF_glWLC"
      },
      "source": [
        "Train-validation-test splits\n",
        "\n",
        "- Split the data into train-validation-test sets \n",
        "- Validation set is used for hyperparameter tuning \n",
        "- Test set is used for the final performance evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7G168j6mMRM"
      },
      "source": [
        "random_seed(1234, True)\n",
        "\n",
        "train_ , test = train_test_split(data, test_size=0.20, random_state=100)\n",
        "train, valid = train_test_split(train_, test_size=0.125, random_state=0)\n",
        "print(train.shape)\n",
        "print(test.shape)\n",
        "print(valid.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt39ZQc7lrgU"
      },
      "source": [
        "### SMILES augmentation for regression task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKaEuRmLlwBx"
      },
      "source": [
        "- For the regression task, a gaussian noise (with mean zero and standard deviation, σg_noise) is added to the labels of the augmented SMILES during the training\n",
        "- The number of augmented SMILES and σg_noise is tuned on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juOd3AxAlsjc"
      },
      "source": [
        "def randomize_smiles(smiles):\n",
        "    m = Chem.MolFromSmiles(smiles)\n",
        "    ans = list(range(m.GetNumAtoms()))\n",
        "    np.random.shuffle(ans)\n",
        "    nm = Chem.RenumberAtoms(m,ans)\n",
        "    return Chem.MolToSmiles(nm, canonical=False, isomericSmiles=True, kekuleSmiles=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0sz0BiQXYSQ"
      },
      "source": [
        "def ee_smiles_augmentation(df, N_rounds, noise):\n",
        "    '''\n",
        "    noise: add gaussion noise to the label\n",
        "    '''\n",
        "    dist_aug = {col_name: [] for col_name in df}\n",
        "\n",
        "    for i in range(df.shape[0]):\n",
        "        for j in range(N_rounds):\n",
        "            dist_aug['smiles'].append(randomize_smiles(df.iloc[i].smiles))\n",
        "            dist_aug['yield'].append(df.iloc[i]['yield'] + np.random.normal(0,noise))\n",
        "    df_aug = pd.DataFrame.from_dict(dist_aug)\n",
        "    df_aug = df_aug.append(df, ignore_index=True)\n",
        "    return df_aug.drop_duplicates('smiles')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAm0iPvNmaBh"
      },
      "source": [
        "random_seed(1234, True)\n",
        "\n",
        "train_aug = ee_smiles_augmentation(train, 100, noise=0.5)\n",
        "print(\"Train_aug: \", train_aug.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUKee64Omafo"
      },
      "source": [
        "### Data pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0rjAJcNmfAg"
      },
      "source": [
        "Define a custom tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxMyks2FmiEn"
      },
      "source": [
        "# Don't include the defalut specific token of fastai, only keep the padding token\n",
        "BOS,EOS,FLD,UNK,PAD = 'xxbos','xxeos','xxfld','xxunk','xxpad'\n",
        "TK_MAJ,TK_UP,TK_REP,TK_WREP = 'xxmaj','xxup','xxrep','xxwrep'\n",
        "defaults.text_spec_tok = [PAD]\n",
        "\n",
        "special_tokens = ['[BOS]', '[C@H]', '[C@@H]','[C@]', '[C@@]','[C-]','[C+]', '[c-]', '[c+]','[cH-]',\n",
        "                   '[nH]', '[N+]', '[N-]', '[n+]', '[n-]' '[NH+]', '[NH2+]',\n",
        "                   '[O-]', '[S+]', '[s+]', '[S-]', '[O+]', '[SH]', '[B-]','[BH2-]', '[BH3-]','[b-]',\n",
        "                   '[PH]','[P+]', '[I+]', \n",
        "                  '[Si]','[SiH2]', '[Se]','[SeH]', '[se]', '[Se+]', '[se+]','[te]','[te+]', '[Te]']\n",
        "\n",
        "class MolTokenizer(BaseTokenizer):\n",
        "    def __init__(self, lang = 'en', special_tokens = special_tokens):\n",
        "        self.lang = lang\n",
        "        self.special_tokens = special_tokens\n",
        "        \n",
        "    def tokenizer(self, smiles):\n",
        "        # add specific token '[BOS]' to represetences the start of SMILES\n",
        "        smiles = '[BOS]' + smiles\n",
        "        regex = '(\\[[^\\[\\]]{1,10}\\])'\n",
        "        char_list = re.split(regex, smiles)\n",
        "        tokens = []\n",
        "        \n",
        "        if self.special_tokens:\n",
        "            for char in char_list:\n",
        "                if char.startswith('['):\n",
        "                    if char in special_tokens:\n",
        "                        tokens.append(str(char))\n",
        "                    else:\n",
        "                        tokens.append('[UNK]')\n",
        "                else:\n",
        "                    chars = [unit for unit in char]\n",
        "                    [tokens.append(i) for i in chars]                    \n",
        "        \n",
        "        if not self.special_tokens:\n",
        "            for char in char_list:\n",
        "                if char.startswith('['):\n",
        "                    tokens.append(str(char))\n",
        "                else:\n",
        "                    chars = [unit for unit in char]\n",
        "                    [tokens.append(i) for i in chars]\n",
        "                \n",
        "        #fix the 'Br' be splited into 'B' and 'r'\n",
        "        if 'B' in tokens:\n",
        "            for index, tok in enumerate(tokens):\n",
        "                if tok == 'B':\n",
        "                    if index < len(tokens)-1: # make sure 'B' is not the last character\n",
        "                        if tokens[index+1] == 'r':\n",
        "                            tokens[index: index+2] = [reduce(lambda i, j: i + j, tokens[index : index+2])]\n",
        "        \n",
        "        #fix the 'Cl' be splited into 'C' and 'l'\n",
        "        if 'l' in tokens:\n",
        "            for index, tok in enumerate(tokens):\n",
        "                if tok == 'l':\n",
        "                    if tokens[index-1] == 'C':\n",
        "                            tokens[index-1: index+1] = [reduce(lambda i, j: i + j, tokens[index-1 : index+1])]\n",
        "        return tokens    \n",
        "    \n",
        "    def add_special_cases(self, toks):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ksMEasDmkXZ"
      },
      "source": [
        "bs = 128\n",
        "tok = Tokenizer(partial(MolTokenizer, special_tokens = special_tokens), n_cpus=6, pre_rules=[], post_rules=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O44sj2QNmrVg"
      },
      "source": [
        "Create a text databunch for regression:\n",
        "\n",
        "- It takes as input the train and validation data\n",
        "- Specify the column containing text data and output\n",
        "- Define the batch size according to the GPU memory available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2-6apdRYCTh"
      },
      "source": [
        "random_seed(1234, True)\n",
        "data_clas = TextClasDataBunch.from_df(path, train_aug, valid, bs=bs, tokenizer=tok, \n",
        "                                          chunksize=50000, text_cols='smiles',label_cols='yield', \n",
        "                                          max_vocab=60000, include_bos=False, min_freq=1, num_workers=0)\n",
        "\n",
        "print(f'Vocab Size: {len(data_clas.vocab.itos)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raLuM4kAm0Yg"
      },
      "source": [
        "### Training the regression model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRoIXo_zm2tA"
      },
      "source": [
        "Create a learner for regression:\n",
        "\n",
        "- Pass the databunch\n",
        "- No pre-trained weights are used, keep pretrained=False\n",
        "- The drop_mult hyperparameter can be tuned\n",
        "- The model is evaluated using RMSE and R-squared value as error metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuf6ZCVaYTx4"
      },
      "source": [
        "random_seed(1234, True)\n",
        "\n",
        "reg_learner = text_classifier_learner(data_clas, AWD_LSTM, pretrained=False, drop_mult=0.0, metrics = [rmse, r2_score])\n",
        "reg_learner.unfreeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0j1CcyHbpAU"
      },
      "source": [
        "# Model architecture\n",
        "reg_learner.model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iswyOeI-nRsD"
      },
      "source": [
        "The regressor is finetuned from scratch. Number of epochs and learning rate are tuned on validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-SOFQWTZju1"
      },
      "source": [
        "random_seed(1234, True)\n",
        "\n",
        "lr = 1e-3\n",
        "\n",
        "reg_learner.fit_one_cycle(15, lr, moms=(0.8,0.7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bHlQVtVnomG"
      },
      "source": [
        "Save the trained learner. It is then later used for prediction on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5M6knDEO0Ph"
      },
      "source": [
        "split_id = 100\n",
        "reg_learner.save(f'{split_id}_reg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsFTSqO6nugW"
      },
      "source": [
        "#### Evaluation on the Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZVolSyVO9yY"
      },
      "source": [
        "preds = []\n",
        "\n",
        "# Randomized SMILES Predictions\n",
        "for i in range(4):\n",
        "    np.random.seed(12*i)\n",
        "    test_aug = test_smiles_augmentation(test,1)\n",
        "    \n",
        "    #model\n",
        "    test_db = TextClasDataBunch.from_df(path, train, test_aug, tokenizer=tok, vocab=data_clas.vocab,\n",
        "                                            text_cols='smiles', label_cols='yield', bs=bs, include_bos=False)\n",
        "    learner = text_classifier_learner(test_db, AWD_LSTM, pretrained=False, drop_mult=0.0, metrics = [r2_score, rmse])\n",
        "    \n",
        "    learner.load(f'{split_id}_reg'); \n",
        "  \n",
        "    #get predictions\n",
        "    pred,lbl = learner.get_preds(ordered=True)\n",
        "    \n",
        "    preds.append(pred)\n",
        "\n",
        "# Canonical SMILES Predictions\n",
        "test_db = TextClasDataBunch.from_df(path, train, test, bs=bs, tokenizer=tok, \n",
        "                              chunksize=50000, text_cols='smiles',label_cols='yield', vocab=data_clas.vocab, max_vocab=60000,\n",
        "                                              include_bos=False)\n",
        "\n",
        "learner = text_classifier_learner(test_db, AWD_LSTM, pretrained=False, drop_mult=0.0, metrics = [r2_score, rmse])\n",
        "\n",
        "learner.load(f'{split_id}_reg');\n",
        "\n",
        "\n",
        "#get predictions\n",
        "pred_canonical,lbl = learner.get_preds(ordered=True)\n",
        "    \n",
        "preds.append(pred_canonical)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNDs9DvEPAE5"
      },
      "source": [
        "print('Test Set (Canonical)')\n",
        "print('RMSE:', root_mean_squared_error(pred_canonical,lbl))\n",
        "print('R2:', r2_score(pred_canonical,lbl))\n",
        "print('MAE:', mean_absolute_error(pred_canonical,lbl))\n",
        "\n",
        "avg_preds = sum(preds)/len(preds)\n",
        "#print('\\n')\n",
        "print('Test Set (Average)')\n",
        "print('RMSE:', root_mean_squared_error(avg_preds,lbl))\n",
        "print('R2:', r2_score(avg_preds,lbl))\n",
        "print('MAE:', mean_absolute_error(avg_preds,lbl))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}