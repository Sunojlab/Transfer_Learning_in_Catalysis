{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TL-m1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCsVEVa7J9kb"
      },
      "source": [
        "## Fine-tuning\n",
        "- This notebook contains code for the fine-tuning of target task regressor using pre-trained weights of general domain language model \n",
        "- The code is adapted from https://github.com/XinhaoLi74/MolPMoFiT/blob/master/notebooks/04_QSAR_Regression.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaI3OCkzKaiA"
      },
      "source": [
        "#### Install RDKit on Google colaboratory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5xgounWJ7PV"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import requests\n",
        "import subprocess\n",
        "import shutil\n",
        "from logging import getLogger, StreamHandler, INFO\n",
        "\n",
        "\n",
        "logger = getLogger(__name__)\n",
        "logger.addHandler(StreamHandler())\n",
        "logger.setLevel(INFO)\n",
        "\n",
        "\n",
        "def install(\n",
        "        chunk_size=4096,\n",
        "        file_name=\"Miniconda3-latest-Linux-x86_64.sh\",\n",
        "        url_base=\"https://repo.continuum.io/miniconda/\",\n",
        "        conda_path=os.path.expanduser(os.path.join(\"~\", \"miniconda\")),\n",
        "        rdkit_version=None,\n",
        "        add_python_path=True,\n",
        "        force=False):\n",
        "    \"\"\"install rdkit from miniconda\n",
        "    ```\n",
        "    import rdkit_installer\n",
        "    rdkit_installer.install()\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    python_path = os.path.join(\n",
        "        conda_path,\n",
        "        \"lib\",\n",
        "        \"python{0}.{1}\".format(*sys.version_info),\n",
        "        \"site-packages\",\n",
        "    )\n",
        "\n",
        "    if add_python_path and python_path not in sys.path:\n",
        "        logger.info(\"add {} to PYTHONPATH\".format(python_path))\n",
        "        sys.path.append(python_path)\n",
        "\n",
        "    if os.path.isdir(os.path.join(python_path, \"rdkit\")):\n",
        "        logger.info(\"rdkit is already installed\")\n",
        "        if not force:\n",
        "            return\n",
        "\n",
        "        logger.info(\"force re-install\")\n",
        "\n",
        "    url = url_base + file_name\n",
        "    python_version = \"{0}.{1}.{2}\".format(*sys.version_info)\n",
        "\n",
        "    logger.info(\"python version: {}\".format(python_version))\n",
        "\n",
        "    if os.path.isdir(conda_path):\n",
        "        logger.warning(\"remove current miniconda\")\n",
        "        shutil.rmtree(conda_path)\n",
        "    elif os.path.isfile(conda_path):\n",
        "        logger.warning(\"remove {}\".format(conda_path))\n",
        "        os.remove(conda_path)\n",
        "\n",
        "    logger.info('fetching installer from {}'.format(url))\n",
        "    res = requests.get(url, stream=True)\n",
        "    res.raise_for_status()\n",
        "    with open(file_name, 'wb') as f:\n",
        "        for chunk in res.iter_content(chunk_size):\n",
        "            f.write(chunk)\n",
        "    logger.info('done')\n",
        "\n",
        "    logger.info('installing miniconda to {}'.format(conda_path))\n",
        "    subprocess.check_call([\"bash\", file_name, \"-b\", \"-p\", conda_path])\n",
        "    logger.info('done')\n",
        "\n",
        "    logger.info(\"installing rdkit\")\n",
        "    subprocess.check_call([\n",
        "        os.path.join(conda_path, \"bin\", \"conda\"),\n",
        "        \"install\",\n",
        "        \"--yes\",\n",
        "        \"-c\", \"rdkit\",\n",
        "        \"python=={}\".format(python_version),\n",
        "        \"rdkit\" if rdkit_version is None else \"rdkit=={}\".format(rdkit_version)])\n",
        "    logger.info(\"done\")\n",
        "\n",
        "    import rdkit\n",
        "    logger.info(\"rdkit-{} installation finished!\".format(rdkit.__version__))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    install()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx-CJc9tKihg"
      },
      "source": [
        "Import the important libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyTyIHjdHLNs"
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "from rdkit import RDLogger \n",
        "RDLogger.DisableLog('rdApp.*') # switch off RDKit warning messages\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from fastai import *\n",
        "from fastai.text import *\n",
        "from fastai.vision import *\n",
        "\n",
        "import numpy as np\n",
        "import threading\n",
        "import random\n",
        "from sklearn.utils import shuffle "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEC4EkeLKrsC"
      },
      "source": [
        "Set the seed value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUnv94uOKs7w"
      },
      "source": [
        "def random_seed(seed_value, use_cuda):\n",
        "    np.random.seed(seed_value) # cpu vars\n",
        "    torch.manual_seed(seed_value) # cpu  vars\n",
        "    random.seed(seed_value) # Python\n",
        "    if use_cuda: \n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value) # gpu vars\n",
        "        torch.backends.cudnn.deterministic = True  #needed\n",
        "        torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTbSO2vAKzny"
      },
      "source": [
        "# Data\n",
        "Mount Google Drive to Google Colab to access the google drive files "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF_7I25AK08_"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7geVIc-7K3dl"
      },
      "source": [
        "# Create a path to save the results\n",
        "\n",
        "data_path = Path('/content/gdrive/My Drive/results')\n",
        "name = 'regressor'\n",
        "path = data_path/name\n",
        "path.mkdir(exist_ok=True, parents=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4M2u9IdK8c_"
      },
      "source": [
        "data = pd.read_csv('/content/gdrive/My Drive/data/target-reactions.csv')\n",
        "print('Dataset:', ee_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM4rjINkLTrY"
      },
      "source": [
        "### Target task regressor fine-tuning on target task LM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wE9jf1YLYZG"
      },
      "source": [
        "Train-validation-test splits\n",
        "\n",
        "- Split the data into train-validation-test sets\n",
        "- Validation set is used for hyperparameter tuning\n",
        "- Test set is used for the final performance evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elnrUwadLX25"
      },
      "source": [
        "random_seed(1234, True)\n",
        "\n",
        "train_ , test = train_test_split(data, test_size=0.20, random_state=100)\n",
        "train, valid = train_test_split(train_, test_size=0.125, random_state=0)\n",
        "print(train.shape)\n",
        "print(test.shape)\n",
        "print(valid.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwKLQ-6HLjJZ"
      },
      "source": [
        "### SMILES augmentation for regression task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLbtFL0BLt2M"
      },
      "source": [
        "- For the regression task, a gaussian noise (with mean zero and standard deviation, σg_noise) is added to the labels of the augmented SMILES during the training\n",
        "- The number of augmented SMILES and σg_noise is tuned on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnMjAIUrHT9M"
      },
      "source": [
        "def randomize_smiles(smiles):\n",
        "    m = Chem.MolFromSmiles(smiles)\n",
        "    ans = list(range(m.GetNumAtoms()))\n",
        "    np.random.shuffle(ans)\n",
        "    nm = Chem.RenumberAtoms(m,ans)\n",
        "    return Chem.MolToSmiles(nm, canonical=False, isomericSmiles=True, kekuleSmiles=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmkPCc0eLoal"
      },
      "source": [
        "def ee_smiles_augmentation(df, N_rounds, noise):\n",
        "    '''\n",
        "    noise: add gaussion noise to the label\n",
        "    '''\n",
        "    dist_aug = {col_name: [] for col_name in df}\n",
        "\n",
        "    for i in range(df.shape[0]):\n",
        "        for j in range(N_rounds):\n",
        "            dist_aug['smiles'].append(randomize_smiles(df.iloc[i].smiles))\n",
        "            dist_aug['ee'].append(df.iloc[i]['ee'] + np.random.normal(0,noise))\n",
        "    df_aug = pd.DataFrame.from_dict(dist_aug)\n",
        "    df_aug = df_aug.append(df, ignore_index=True)\n",
        "    return df_aug.drop_duplicates('smiles')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yh5O6OG0I2r1"
      },
      "source": [
        "random_seed(1234, True)\n",
        "\n",
        "train_aug = ee_smiles_augmentation(train, 100, noise=0.5)\n",
        "print(\"Train_aug: \", train_aug.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEc8kZScMFYz"
      },
      "source": [
        "### Data pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTE_dUSDMLsT"
      },
      "source": [
        "Define a custom tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XMdhvKTMHtT"
      },
      "source": [
        "# Don't include the defalut specific token of fastai, only keep the padding token\n",
        "BOS,EOS,FLD,UNK,PAD = 'xxbos','xxeos','xxfld','xxunk','xxpad'\n",
        "TK_MAJ,TK_UP,TK_REP,TK_WREP = 'xxmaj','xxup','xxrep','xxwrep'\n",
        "defaults.text_spec_tok = [PAD]\n",
        "\n",
        "special_tokens = ['[BOS]', '[C@H]', '[C@@H]','[C@]', '[C@@]','[C-]','[C+]', '[c-]', '[c+]','[cH-]',\n",
        "                   '[nH]', '[N+]', '[N-]', '[n+]', '[n-]' '[NH+]', '[NH2+]',\n",
        "                   '[O-]', '[S+]', '[s+]', '[S-]', '[O+]', '[SH]', '[B-]','[BH2-]', '[BH3-]','[b-]',\n",
        "                   '[PH]','[P+]', '[I+]', \n",
        "                  '[Si]','[SiH2]', '[Se]','[SeH]', '[se]', '[Se+]', '[se+]','[te]','[te+]', '[Te]']\n",
        "\n",
        "class MolTokenizer(BaseTokenizer):\n",
        "    def __init__(self, lang = 'en', special_tokens = special_tokens):\n",
        "        self.lang = lang\n",
        "        self.special_tokens = special_tokens\n",
        "        \n",
        "    def tokenizer(self, smiles):\n",
        "        # add specific token '[BOS]' to represetences the start of SMILES\n",
        "        smiles = '[BOS]' + smiles\n",
        "        regex = '(\\[[^\\[\\]]{1,10}\\])'\n",
        "        char_list = re.split(regex, smiles)\n",
        "        tokens = []\n",
        "        \n",
        "        if self.special_tokens:\n",
        "            for char in char_list:\n",
        "                if char.startswith('['):\n",
        "                    if char in special_tokens:\n",
        "                        tokens.append(str(char))\n",
        "                    else:\n",
        "                        tokens.append('[UNK]')\n",
        "                else:\n",
        "                    chars = [unit for unit in char]\n",
        "                    [tokens.append(i) for i in chars]                    \n",
        "        \n",
        "        if not self.special_tokens:\n",
        "            for char in char_list:\n",
        "                if char.startswith('['):\n",
        "                    tokens.append(str(char))\n",
        "                else:\n",
        "                    chars = [unit for unit in char]\n",
        "                    [tokens.append(i) for i in chars]\n",
        "                \n",
        "        #fix the 'Br' be splited into 'B' and 'r'\n",
        "        if 'B' in tokens:\n",
        "            for index, tok in enumerate(tokens):\n",
        "                if tok == 'B':\n",
        "                    if index < len(tokens)-1: # make sure 'B' is not the last character\n",
        "                        if tokens[index+1] == 'r':\n",
        "                            tokens[index: index+2] = [reduce(lambda i, j: i + j, tokens[index : index+2])]\n",
        "        \n",
        "        #fix the 'Cl' be splited into 'C' and 'l'\n",
        "        if 'l' in tokens:\n",
        "            for index, tok in enumerate(tokens):\n",
        "                if tok == 'l':\n",
        "                    if tokens[index-1] == 'C':\n",
        "                            tokens[index-1: index+1] = [reduce(lambda i, j: i + j, tokens[index-1 : index+1])]\n",
        "        return tokens    \n",
        "    \n",
        "    def add_special_cases(self, toks):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJq4PNusMRkE"
      },
      "source": [
        "bs = 128\n",
        "tok = Tokenizer(partial(MolTokenizer, special_tokens = special_tokens), n_cpus=6, pre_rules=[], post_rules=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4ZC6MOTJCDv"
      },
      "source": [
        "Adpot the encoder of the general domain LM according to the target dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imddL80mN7kg"
      },
      "source": [
        "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning) \n",
        "random_seed(1234, True)\n",
        "\n",
        "lm_vocab = TextLMDataBunch.from_df(path, train_aug, valid, bs=bs, tokenizer=tok, \n",
        "                              chunksize=50000, text_cols=0,label_cols=1, max_vocab=60000, include_bos=False, min_freq=1, num_workers=0)\n",
        "print(f'Vocab Size: {len(lm_vocab.vocab.itos)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Esr8ehTFN-FG"
      },
      "source": [
        "pretrained_model_path = Path('/content/gdrive/My Drive/results/MSPM/models')\n",
        "pretrained_fnames = ['MSPM_wt', 'MSPM_vocab']\n",
        "fnames = [pretrained_model_path/f'{fn}.{ext}' for fn,ext in zip(pretrained_fnames, ['pth', 'pkl'])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeKcUMrOOBGc"
      },
      "source": [
        "random_seed(1234, True)\n",
        "\n",
        "lm_learner = language_model_learner(qsar_vocab, AWD_LSTM, drop_mult=0.2, pretrained=False)\n",
        "lm_learner = lm_learner.load_pretrained(*fnames)\n",
        "lm_learner.freeze()\n",
        "lm_learner.save_encoder(f'lm_encoder')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRwAirrxOEXF"
      },
      "source": [
        "Create a text databunch for regression:\n",
        "\n",
        "- It takes as input the train and validation data\n",
        "- Pass the vocab of the pre-trained LM as defined in the previous step\n",
        "- Specify the column containing text data and output\n",
        "- Define the batch size according to the GPU memory available\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IpahKD-ODRF"
      },
      "source": [
        "random_seed(1234, True)\n",
        "\n",
        "data_clas = TextClasDataBunch.from_df(path, train_aug, valid, bs=bs, tokenizer=tok, \n",
        "                                          chunksize=50000, text_cols='smiles',label_cols='ee', \n",
        "                                          vocab=lm_vocab.vocab, max_vocab=60000, include_bos=False, min_freq=1, num_workers=0)\n",
        "\n",
        "print(f'Vocab Size: {len(data_clas.vocab.itos)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kk2kZ9V5OUoy"
      },
      "source": [
        "### Training the regression model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0Vs1CL4NRkm"
      },
      "source": [
        "Create a learner for regression:\n",
        "\n",
        "- Pass the databunch\n",
        "- Load the encoder of the pre-trained LM\n",
        "- The drop_mult hyperparameter can be tuned\n",
        "- The model is evaluated using RMSE and R-squared value as error metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70eCKGwwOX13"
      },
      "source": [
        "random_seed(1234, True)\n",
        "\n",
        "reg_learner = text_classifier_learner(data_clas, AWD_LSTM, pretrained=False, drop_mult=0.2, metrics = [r2_score, rmse])\n",
        "reg_learner.load_encoder(f'lm_encoder')\n",
        "reg_learner.freeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPSxFKFr3uwH"
      },
      "source": [
        "# Model architecture\n",
        "reg_learner.model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdRW-jymNc1K"
      },
      "source": [
        "The regressor is fine-tuned using gradual unfreezing method in four steps:\n",
        "\n",
        "- the regressor\n",
        "- the regressor and the final LSTM layer\n",
        "- the regressor and the last two LSTM layers, and\n",
        "- the full model\n",
        "\n",
        "Number of epochs and learning rate in each of these steps are tuned"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58Epn7bgOa8l"
      },
      "source": [
        "random_seed(1234, True)\n",
        "\n",
        "reg_learner.fit_one_cycle(5, lr, moms=(0.8,0.7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-P-nB10COdNb"
      },
      "source": [
        "random_seed(1234, True)\n",
        "\n",
        "reg_learner.freeze_to(-2)\n",
        "reg_learner.fit_one_cycle(6, 1e-2, moms=(0.8,0.7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4z1JzkROfoA"
      },
      "source": [
        "random_seed(1234, True)\n",
        "\n",
        "reg_learner.freeze_to(-3)\n",
        "reg_learner.fit_one_cycle(6, 1e-3, moms=(0.8,0.7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmYF1JesOhrN"
      },
      "source": [
        "random_seed(1234, True)\n",
        "\n",
        "reg_learner.unfreeze()\n",
        "reg_learner.fit_one_cycle(6, 1e-3, moms=(0.8,0.7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGt4qRezNnEZ"
      },
      "source": [
        "The regressor can also be fine-tuned all at once without any frozen weights (i.e., no gradual unfreezing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG90CfeKNpmZ"
      },
      "source": [
        "random_seed(1234, True)\n",
        "\n",
        "reg_learner.unfreeze()\n",
        "reg_learner.fit_one_cycle(10, 1e-3, moms=(0.8,0.7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOUGPj9sneFw"
      },
      "source": [
        "Save the trained learner. It is then later used for prediction on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hVnvsU9Oj2z"
      },
      "source": [
        "split_id = 1\n",
        "reg_learner.save(f'{split_id}_reg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mv-aCNtOmwO"
      },
      "source": [
        "#### Evaluation on the Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPfIu6o4OpCV"
      },
      "source": [
        "def test_smiles_augmentation(df, N_rounds):\n",
        "    dist_aug = {col_name: [] for col_name in df}\n",
        "\n",
        "    for i in range(df.shape[0]):\n",
        "        for j in range(N_rounds):\n",
        "            dist_aug['smiles'].append(randomize_smiles(df.iloc[i].smiles))\n",
        "            dist_aug['ee'].append(df.iloc[i]['ee'])\n",
        "    df_aug = pd.DataFrame.from_dict(dist_aug)\n",
        "    \n",
        "    return pd.DataFrame.from_dict(dist_aug)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cin_ygfT9Plp"
      },
      "source": [
        "The test set performance is evaluated using the predictions based on the canonical SMILES as well as that employing test-time augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GelpIHJQOsG9"
      },
      "source": [
        "preds = []\n",
        "\n",
        "# Randomized SMILES Predictions\n",
        "for i in range(4):\n",
        "    np.random.seed(12*i)\n",
        "    test_aug = test_smiles_augmentation(test,1)\n",
        "    \n",
        "    #model\n",
        "    test_db = TextClasDataBunch.from_df(path, train, test_aug, tokenizer=tok, vocab=qsar_vocab.vocab,\n",
        "                                            text_cols='smiles', label_cols='ee', bs=bs, include_bos=False)\n",
        "    \n",
        "    learner = text_classifier_learner(test_db, AWD_LSTM, pretrained=False, drop_mult=0.2, metrics = [r2_score, root_mean_squared_error])\n",
        "    #print(test_db)\n",
        "    learner.load(f'{split_id}_reg'); \n",
        "  \n",
        "    #get predictions\n",
        "    pred,lbl = learner.get_preds(ds_type=DatasetType.Valid)\n",
        "    \n",
        "    preds.append(pred)\n",
        "\n",
        "# Canonical SMILES Predictions\n",
        "test_db = TextClasDataBunch.from_df(path, train, test, bs=bs, tokenizer=tok, \n",
        "                              chunksize=50000, text_cols='smiles',label_cols='ee', vocab=qsar_vocab.vocab, max_vocab=60000,\n",
        "                                              include_bos=False)\n",
        "\n",
        "learner = text_classifier_learner(test_db, AWD_LSTM, pretrained=False, drop_mult=0.2, metrics = [r2_score, root_mean_squared_error])\n",
        "\n",
        "learner.load(f'{split_id}_reg');\n",
        "\n",
        "\n",
        "#get predictions\n",
        "pred_canonical,lbl = learner.get_preds(ds_type=DatasetType.Valid)\n",
        "    \n",
        "preds.append(pred_canonical)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTe8N2HmOu7Q"
      },
      "source": [
        "print('Test Set (Canonical)')\n",
        "print('RMSE:', root_mean_squared_error(pred_canonical,lbl))\n",
        "print('MAE:', mean_absolute_error(pred_canonical,lbl))\n",
        "print('R2:', r2_score(pred_canonical,lbl))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyvqaP6-Ox-I"
      },
      "source": [
        "avg_preds = sum(preds)/len(preds)\n",
        "#print('\\n')\n",
        "print('Test Set (Average)')\n",
        "print('RMSE:', root_mean_squared_error(avg_preds,lbl))\n",
        "print('MAE:', mean_absolute_error(avg_preds,lbl))\n",
        "print('R2:', r2_score(avg_preds,lbl))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}