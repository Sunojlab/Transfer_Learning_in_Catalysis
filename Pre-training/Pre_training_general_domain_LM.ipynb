{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pre-training_general_domain_LM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNbW2Sz9jvhS"
      },
      "source": [
        "# Pre-training\n",
        "- This notebook contains the code for pre-training the general domain language model on 1 million molecules from the ChEMBL database using the SMILES representation\n",
        "- The code is adapted from https://github.com/XinhaoLi74/MolPMoFiT/blob/master/notebooks/01_MSPM_Pretraining.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v-PQRQElIFM"
      },
      "source": [
        "### Install RDKit on Google colaboratory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD_DN10aj1T3"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import requests\n",
        "import subprocess\n",
        "import shutil\n",
        "from logging import getLogger, StreamHandler, INFO\n",
        "\n",
        "\n",
        "logger = getLogger(__name__)\n",
        "logger.addHandler(StreamHandler())\n",
        "logger.setLevel(INFO)\n",
        "\n",
        "\n",
        "def install(\n",
        "        chunk_size=4096,\n",
        "        file_name=\"Miniconda3-latest-Linux-x86_64.sh\",\n",
        "        url_base=\"https://repo.continuum.io/miniconda/\",\n",
        "        conda_path=os.path.expanduser(os.path.join(\"~\", \"miniconda\")),\n",
        "        rdkit_version=None,\n",
        "        add_python_path=True,\n",
        "        force=False):\n",
        "    \"\"\"install rdkit from miniconda\n",
        "    ```\n",
        "    import rdkit_installer\n",
        "    rdkit_installer.install()\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    python_path = os.path.join(\n",
        "        conda_path,\n",
        "        \"lib\",\n",
        "        \"python{0}.{1}\".format(*sys.version_info),\n",
        "        \"site-packages\",\n",
        "    )\n",
        "\n",
        "    if add_python_path and python_path not in sys.path:\n",
        "        logger.info(\"add {} to PYTHONPATH\".format(python_path))\n",
        "        sys.path.append(python_path)\n",
        "\n",
        "    if os.path.isdir(os.path.join(python_path, \"rdkit\")):\n",
        "        logger.info(\"rdkit is already installed\")\n",
        "        if not force:\n",
        "            return\n",
        "\n",
        "        logger.info(\"force re-install\")\n",
        "\n",
        "    url = url_base + file_name\n",
        "    python_version = \"{0}.{1}.{2}\".format(*sys.version_info)\n",
        "\n",
        "    logger.info(\"python version: {}\".format(python_version))\n",
        "\n",
        "    if os.path.isdir(conda_path):\n",
        "        logger.warning(\"remove current miniconda\")\n",
        "        shutil.rmtree(conda_path)\n",
        "    elif os.path.isfile(conda_path):\n",
        "        logger.warning(\"remove {}\".format(conda_path))\n",
        "        os.remove(conda_path)\n",
        "\n",
        "    logger.info('fetching installer from {}'.format(url))\n",
        "    res = requests.get(url, stream=True)\n",
        "    res.raise_for_status()\n",
        "    with open(file_name, 'wb') as f:\n",
        "        for chunk in res.iter_content(chunk_size):\n",
        "            f.write(chunk)\n",
        "    logger.info('done')\n",
        "\n",
        "    logger.info('installing miniconda to {}'.format(conda_path))\n",
        "    subprocess.check_call([\"bash\", file_name, \"-b\", \"-p\", conda_path])\n",
        "    logger.info('done')\n",
        "\n",
        "    logger.info(\"installing rdkit\")\n",
        "    subprocess.check_call([\n",
        "        os.path.join(conda_path, \"bin\", \"conda\"),\n",
        "        \"install\",\n",
        "        \"--yes\",\n",
        "        \"-c\", \"rdkit\",\n",
        "        \"python=={}\".format(python_version),\n",
        "        \"rdkit\" if rdkit_version is None else \"rdkit=={}\".format(rdkit_version)])\n",
        "    logger.info(\"done\")\n",
        "\n",
        "    import rdkit\n",
        "    logger.info(\"rdkit-{} installation finished!\".format(rdkit.__version__))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    install()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaRgkZc6mhCa"
      },
      "source": [
        "Import the important libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfY8NjI4mlLY"
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "\n",
        "from fastai import *\n",
        "from fastai.text import *\n",
        "\n",
        "import numpy as np\n",
        "import threading"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBVAbgRHl3di"
      },
      "source": [
        "## Data\n",
        "Mount Google Drive to Google Colab to access the google drive files "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcBa6cbql2s7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weoiDjStnCiL"
      },
      "source": [
        "Load train and test data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSaoCNgMnQVI"
      },
      "source": [
        "train = pd.read_csv('/content/gdrive/My Drive/data/ChemBL-LM_train.csv')\n",
        "valid = pd.read_csv('/content/gdrive/My Drive/data/ChemBL-LM_val.csv')\n",
        "train.shape, valid.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gjrvr8IGmReA"
      },
      "source": [
        "# Create a path to save the resluts\n",
        "result_path = Path('/content/gdrive/My Drive/results')\n",
        "name = 'pre-trained'\n",
        "path = result_path/name\n",
        "path.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "mdl_path = path/'models'\n",
        "mdl_path.mkdir(exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNy-sm82nalD"
      },
      "source": [
        "## SMILES augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lx3Cr4_cm4LJ"
      },
      "source": [
        "def randomize_smiles(smiles):\n",
        "    m = Chem.MolFromSmiles(smiles)\n",
        "    ans = list(range(m.GetNumAtoms()))\n",
        "    np.random.shuffle(ans)\n",
        "    nm = Chem.RenumberAtoms(m,ans)\n",
        "    return Chem.MolToSmiles(nm, canonical=False, isomericSmiles=True, kekuleSmiles=False)\n",
        "\n",
        "def smiles_augmentation(df, N_rounds):\n",
        "    dist_aug = {col_name: [] for col_name in df}\n",
        "\n",
        "    for i in range(df.shape[0]):\n",
        "        for j in range(N_rounds):\n",
        "            dist_aug['SMILES'].append(randomize_smiles(df.iloc[i].SMILES))\n",
        "            dist_aug['canonical'].append('no')\n",
        "\n",
        "    df_aug = pd.DataFrame.from_dict(dist_aug)\n",
        "    \n",
        "    #merge with original df\n",
        "    df = pd.concat([df, df_aug], sort=False).reset_index(drop=True)\n",
        "    #shuffle the data\n",
        "    df = df.reindex(np.random.permutation(df.index))\n",
        "    return pd.DataFrame.from_dict(df).drop_duplicates('SMILES')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUgIUWEnnwmO"
      },
      "source": [
        "The randomized SMILES are used for data augmentation. Each SMILES is augmented by four other SMILES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddOT1pZqnk_P"
      },
      "source": [
        "%%time\n",
        "train_aug = smiles_augmentation(train, 4)\n",
        "valid_aug = smiles_augmentation(valid, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2jFVvJqoNW2"
      },
      "source": [
        "## Data pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGAHv4iPoVi-"
      },
      "source": [
        "Define a custom tokenizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZEVXtXCViUg"
      },
      "source": [
        "# Don't include the defalut specific token of fastai, only keep the padding token\n",
        "BOS,EOS,FLD,UNK,PAD = 'xxbos','xxeos','xxfld','xxunk','xxpad'\n",
        "TK_MAJ,TK_UP,TK_REP,TK_WREP = 'xxmaj','xxup','xxrep','xxwrep'\n",
        "defaults.text_spec_tok = [PAD]\n",
        "\n",
        "special_tokens = ['[BOS]', '[C@H]', '[C@@H]','[C@]', '[C@@]','[C-]','[C+]', '[c-]', '[c+]','[cH-]',\n",
        "                   '[nH]', '[N+]', '[N-]', '[n+]', '[n-]' '[NH+]', '[NH2+]',\n",
        "                   '[O-]', '[S+]', '[s+]', '[S-]', '[O+]', '[SH]', '[B-]','[BH2-]', '[BH3-]','[b-]',\n",
        "                   '[PH]','[P+]', '[I+]', \n",
        "                  '[Si]','[SiH2]', '[Se]','[SeH]', '[se]', '[Se+]', '[se+]','[te]','[te+]', '[Te]']\n",
        "\n",
        "class MolTokenizer(BaseTokenizer):\n",
        "    def __init__(self, lang = 'en', special_tokens = special_tokens):\n",
        "        self.lang = lang\n",
        "        self.special_tokens = special_tokens\n",
        "        \n",
        "    def tokenizer(self, smiles):\n",
        "        # add specific token '[BOS]' to represetences the start of SMILES\n",
        "        smiles = '[BOS]' + smiles\n",
        "        regex = '(\\[[^\\[\\]]{1,10}\\])'\n",
        "        char_list = re.split(regex, smiles)\n",
        "        tokens = []\n",
        "        \n",
        "        if self.special_tokens:\n",
        "            for char in char_list:\n",
        "                if char.startswith('['):\n",
        "                    if char in special_tokens:\n",
        "                        tokens.append(str(char))\n",
        "                    else:\n",
        "                        tokens.append('[UNK]')\n",
        "                else:\n",
        "                    chars = [unit for unit in char]\n",
        "                    [tokens.append(i) for i in chars]                    \n",
        "        \n",
        "        if not self.special_tokens:\n",
        "            for char in char_list:\n",
        "                if char.startswith('['):\n",
        "                    tokens.append(str(char))\n",
        "                else:\n",
        "                    chars = [unit for unit in char]\n",
        "                    [tokens.append(i) for i in chars]\n",
        "                \n",
        "        #fix the 'Br' be splited into 'B' and 'r'\n",
        "        if 'B' in tokens:\n",
        "            for index, tok in enumerate(tokens):\n",
        "                if tok == 'B':\n",
        "                    if index < len(tokens)-1: # make sure 'B' is not the last character\n",
        "                        if tokens[index+1] == 'r':\n",
        "                            tokens[index: index+2] = [reduce(lambda i, j: i + j, tokens[index : index+2])]\n",
        "        \n",
        "        #fix the 'Cl' be splited into 'C' and 'l'\n",
        "        if 'l' in tokens:\n",
        "            for index, tok in enumerate(tokens):\n",
        "                if tok == 'l':\n",
        "                    if tokens[index-1] == 'C':\n",
        "                            tokens[index-1: index+1] = [reduce(lambda i, j: i + j, tokens[index-1 : index+1])]\n",
        "        return tokens    \n",
        "    \n",
        "    def add_special_cases(self, toks):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfbDtRBHWyRg"
      },
      "source": [
        "# Tokenizer\n",
        "tok = Tokenizer(partial(MolTokenizer, special_tokens = special_tokens), n_cpus=6, pre_rules=[], post_rules=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqUe_jsOotoW"
      },
      "source": [
        "Create a text databunch for language modeling:\n",
        "- It takes as input the train and validation data\n",
        "- Pass the custom tokenizer defined in the previous step\n",
        "- Specify the column containing text data\n",
        "- Define the batch size according to the GPU memory available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poaB4R5IW1_L"
      },
      "source": [
        "%%time\n",
        "bs = 128  #batch size\n",
        "\n",
        "data = TextLMDataBunch.from_df(path, train_aug, valid_aug, bs=bs, tokenizer=tok, chunksize=50000, text_cols=0, max_vocab=60000, include_bos=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyQx14UpW6j8"
      },
      "source": [
        "# Save the databunch \n",
        "data.save(f'{name}_databunch')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlzqOtVWW-TX"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA7veDl90CBt"
      },
      "source": [
        "Load the databunch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVCPWug6XB3T"
      },
      "source": [
        "bs = 128 # batch size\n",
        "data_lm = load_data(path, f'{name}_databunch', bs=bs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DU4C8tPJ0JKN"
      },
      "source": [
        "Create a learner for language modeling:\n",
        "- As the model is trained from scratch, use pretrained=False\n",
        "- Pass the text databunch loaded in the previous step\n",
        "- Drop_mult is a hyperparameter that can be tuned"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBO5DQkwcXB4"
      },
      "source": [
        "learner = language_model_learner(data_lm, AWD_LSTM, drop_mult=1, pretrained=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45e2Ghn-cZdw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "f4acd558-62ce-4882-93d1-6b5fbf609257"
      },
      "source": [
        "# Model architecture\n",
        "learner.model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SequentialRNN(\n",
              "  (0): AWD_LSTM(\n",
              "    (encoder): Embedding(80, 400, padding_idx=1)\n",
              "    (encoder_dp): EmbeddingDropout(\n",
              "      (emb): Embedding(80, 400, padding_idx=1)\n",
              "    )\n",
              "    (rnns): ModuleList(\n",
              "      (0): WeightDropout(\n",
              "        (module): LSTM(400, 1152, batch_first=True)\n",
              "      )\n",
              "      (1): WeightDropout(\n",
              "        (module): LSTM(1152, 1152, batch_first=True)\n",
              "      )\n",
              "      (2): WeightDropout(\n",
              "        (module): LSTM(1152, 400, batch_first=True)\n",
              "      )\n",
              "    )\n",
              "    (input_dp): RNNDropout()\n",
              "    (hidden_dps): ModuleList(\n",
              "      (0): RNNDropout()\n",
              "      (1): RNNDropout()\n",
              "      (2): RNNDropout()\n",
              "    )\n",
              "  )\n",
              "  (1): LinearDecoder(\n",
              "    (decoder): Linear(in_features=400, out_features=80, bias=True)\n",
              "    (output_dp): RNNDropout()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e3fAfnm3jrZ"
      },
      "source": [
        "Train the model using fit_one_cycle\n",
        "- the first hyperparameter is number of epochs\n",
        "- the second hyperparameter is learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tgc68uC4cc3-"
      },
      "source": [
        "lr = 3e-3\n",
        "lr *= bs/48  # Scale learning rate by batch size\n",
        "\n",
        "learner.unfreeze()\n",
        "learner.fit_one_cycle(10, lr, moms=(0.8,0.7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDBhi1vq4CPn"
      },
      "source": [
        "Save the weights and vocabulary of the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzsXP63Jci2V"
      },
      "source": [
        "lm_fns = [f'{name}_wt', f'{name}_vocab']\n",
        "\n",
        "learner.save(lm_fns[0], with_opt=False)\n",
        "learner.data.vocab.save(mdl_path/(lm_fns[1] + '.pkl'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}